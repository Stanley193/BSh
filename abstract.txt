Field Programmable Gate Array (FPGA) is a kind of Very Large Scale Intergrated circuits (VLSI). These days, FPGAs have already become one of the most verification tools in digital intergrated circuits, for they embed vast number of logic resources, memory resources, digital signal processing (DSP) units and programmable interconnecting resources. Completely configured FPGAs can even be put into markets as products, when the application envirnment requires relatively high flexibility and energy performance. As one of the newest kinds of VLSI, the huge number of logic and computation resources that arranged in the intergrated circuits, make FPGAs' array architecture  possesses great parallel computing advantage.On the other hand, among a lot of algorithms that can be accelerated making use of parallel computing, the Convolutional Nerual Netwark (CNN) is one of the state of art algorithms in computer vision. In image recognition, CNN algorithms achieve very low error rate, besides, CNN algorithms over come some considerable deformations in shapes and displacement of the target.

By studying the embedded resources in FPGAs and their structures, and analizing the operations in the CNN algorithms, we notice that the array structures of the logic and computation resources in the FPGAs are extremely samilar to the repeated and independent operations in CNN. Thus, mapping a CNN into an FPGA will extract the parallel advantage to the most extent. However, considering the limitations of a singal logic unit in FPGAs, this paper pay special attention to the designing of CNN basic operation unit by using the large number of DSP units embedded in FPGAs.

By studying CNNs carefully, this paper perpose the operation level: from feature map to feature map. This level is comparatively higher than the CNN basic operations: convolutions, nonlinear activation functions and subsamplings. The operations from feature map to feature map is highly repeated, and an FPGA module can be adopted to many different layers in the whole convolutional nerual netwark without much modification. FPGA modules realizing this level of operation can achieve parallel accellaration by copying themselves. On the other hand, these level of operations are so samilar that the FPGA module can be reused and completes the computation in sieral pattern, which saving the hardware resources. As a result, mapping this level of operation in FPGAs obtains a design of which the parallel degree can be adjusted flexibly. This is the tread off between area and speed.

When mapping the basic module into an FPGA, the 2D convoluters, nonlinear functions and the subsampling units are the key basic modules. This paper discribs several FPGA schemes of each basic module. They meet the demanding of the tread off between area and speed, for either these schemes are parallel or sieral designed.